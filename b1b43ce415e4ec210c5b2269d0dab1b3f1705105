{
  "comments": [
    {
      "key": {
        "uuid": "155c9b0f_38ce4d7e",
        "filename": "tests/unit/test_v3.py",
        "patchSetId": 5
      },
      "lineNbr": 6953,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "Why this change?",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "be638a43_0f66fcb4",
        "filename": "tests/unit/test_v3.py",
        "patchSetId": 5
      },
      "lineNbr": 6953,
      "author": {
        "id": 27952
      },
      "writtenOn": "2021-05-04T11:28:40Z",
      "side": 1,
      "message": "This was not intended. Maybe some leftover from the old version of this change after the rebasing.",
      "parentUuid": "155c9b0f_38ce4d7e",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "6c4872be_2a092a12",
        "filename": "zuul/executor/client.py",
        "patchSetId": 5
      },
      "lineNbr": 200,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "Why are we adding a new zuul variable (for users to use in jobs) here?\n\nOkay, I found the answer below -- we check to see if we exceed the attempts on the executor now because it\u0027s processing the autoholds.  We should do one of two things here:\n\n1) Add this to params instead of params[\u0027zuul\u0027] so that it\u0027s hidden from users.\nor\n2) Document it as an additional zuul job variable.",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9fa92225_4abd5d9c",
        "filename": "zuul/executor/client.py",
        "patchSetId": 5
      },
      "lineNbr": 200,
      "author": {
        "id": 27952
      },
      "writtenOn": "2021-05-04T11:28:40Z",
      "side": 1,
      "message": "I thought it\u0027s a good idea to store them in the same place as the attempts. But I get the point about user visibility.\n\nI don\u0027t have a strong opinion on this, but the easiest would be just move it to the params rather than params[\"zuul\"].",
      "parentUuid": "6c4872be_2a092a12",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "7b48f317_3f5f805c",
        "filename": "zuul/executor/server.py",
        "patchSetId": 5
      },
      "lineNbr": 3131,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "How about we either put this method on the AnsibleJob, or hand the nodeset to it in the constructor (on line 3114 above)?  Even once the build is in ZK, we still need a place to store the kazoo API lock object; we could put in on the local API representation of the build (that may be what you were thinking) but I think the AnsibleJob may be a good place that works with both systems.",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "83476582_0487f1e6",
        "filename": "zuul/executor/server.py",
        "patchSetId": 5
      },
      "lineNbr": 3131,
      "author": {
        "id": 27952
      },
      "writtenOn": "2021-05-04T11:28:40Z",
      "side": 1,
      "message": "Yes, my plan was to use the local API representation of the build, but I get your point about the AnsibleJob. I will try that out.",
      "parentUuid": "7b48f317_3f5f805c",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e3d61232_8b9f6f79",
        "filename": "zuul/executor/server.py",
        "patchSetId": 5
      },
      "lineNbr": 3450,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "Capitalize and log at info (this is useful even when not debugging).",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "c6ffe683_5e13b479",
        "filename": "zuul/model.py",
        "patchSetId": 5
      },
      "lineNbr": 759,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "I\u0027d prefer to avoid changing unrelated lines on complicated changes like this, but if you really want to change these lines, please switch from items() to values() in order to avoid the \u0027_\u0027 variable.",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "55d3d4b2_efab7ea4",
        "filename": "zuul/nodepool.py",
        "patchSetId": 5
      },
      "lineNbr": 181,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "A gearman job seems like the wrong thing to be passing into this.  How about either the AnsibleJob, or we could create a temporary \"Build\" class that has all the same info (which we construct from the job arguments), then switch it for the real zuul.model.Build later once we have that in ZK?\n\nIncidentally, this change has convinced me that once we have the build in ZK, we should remove the arguments from the BuildRequest and just reference the Build from it.",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "a6f4e92c_431f4226",
        "filename": "zuul/nodepool.py",
        "patchSetId": 5
      },
      "lineNbr": 181,
      "author": {
        "id": 27952
      },
      "writtenOn": "2021-05-04T11:28:40Z",
      "side": 1,
      "message": "\u003e A gearman job seems like the wrong thing to be passing into this\n\nI thought the same, but it was the least thing I could provide without changing too much. In combination with your comment on https://review.opendev.org/c/zuul/zuul/+/774610/5/zuul/executor/server.py#3131 it might be a start to use the AnsibleJob here instead and see where this leads us. As long as the build itself is not in ZK, I would like to avoid having another \"build-like\" object with the same params in another stack of changes.",
      "parentUuid": "55d3d4b2_efab7ea4",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "01403e4b_11da3f2d",
        "filename": "zuul/nodepool.py",
        "patchSetId": 5
      },
      "lineNbr": 442,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "I think this question may be moot; see below.",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9d852b83_e3f2f227",
        "filename": "zuul/nodepool.py",
        "patchSetId": 5
      },
      "lineNbr": 454,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-03T23:18:44Z",
      "side": 1,
      "message": "If you delete a node request without locking the nodes, the Nodepool will notice that, lock the nodes, and reallocate them.  It may mostly work most of the time because that\u0027s a periodic action, but I think every few minutes, you would probably see nodepool go through and reallocate a bunch of node requests right before executors tried to start.\n\nIn the nodepool protocol, nodes must be locked before the request is deleted.  So I think what we need to do is leave this method as-is, and remove its invocation from the scheduler, then let the pipeline manager see that the request is fulfilled and therefore it\u0027s ready to submit a build request.  Then the executor should call this method instead of the \"lock + use\" that you currently have.",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "71732e67_e604fa70",
        "filename": "zuul/nodepool.py",
        "patchSetId": 5
      },
      "lineNbr": 454,
      "author": {
        "id": 27952
      },
      "writtenOn": "2021-05-04T11:28:40Z",
      "side": 1,
      "message": "Ok. This would mean that we remove this call from the scheduler https://opendev.org/zuul/zuul/src/branch/master/zuul/scheduler.py#L1796 but have to find another way to tell the pipeline manager that the job is ready for execution?",
      "parentUuid": "9d852b83_e3f2f227",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "23357efb_b608e296",
        "filename": "zuul/nodepool.py",
        "patchSetId": 5
      },
      "lineNbr": 454,
      "author": {
        "id": 27952
      },
      "writtenOn": "2021-05-06T12:45:06Z",
      "side": 1,
      "message": "I tried to move the acceptNodes() call to the executor side, but I\u0027m a little stuck there.\n\nThe main reason is: To call the acceptNodes() on the executor, I would have to provide the NodeRequest object to the executor. Although most of the NodeRequest data is stored in ZooKeeper, there is a lot ongoing on the NodeRequest object itself which is not persisted to ZooKeeper. Thus, I would have to fully serialize the NodeRequest first to be able to restore a \"rich\" NodeRequest object on the executor side that can be used to interact with the local NodePool API.\n\nI have already started with this in a follow-up change (not pushed to gerrit yet) where I switch the NodePool API from the \"in-place NodeRequest update\" (which updates the local NodeRequest object with fresh data from ZooKeeper) to a TreeCache. But I would like to keep that change on top of this one, because I think it\u0027s simpler to update the NodePool API when the node locking is done in the executor.\n\nI can try to move parts out of the follow-up change into this one to make the NodeRequest fully serializable. Maybe doing that is also worth a change on its own, but I currently cannot see the whole impact of this change. When this is done, we might be able to keep the acceptNodes() method like it is.\n\nI hope this was somewhat understandable. Please let me know if anything is unclear or if you have a better idea. Maybe I\u0027m also missing some important point.",
      "parentUuid": "71732e67_e604fa70",
      "revId": "b1b43ce415e4ec210c5b2269d0dab1b3f1705105",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": true
    }
  ]
}