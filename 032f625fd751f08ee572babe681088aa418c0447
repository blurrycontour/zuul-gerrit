{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "b30f066f_1bbe8239",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "Love.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "97b30b6b_33094f85",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 8,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "nit: a/the plan",
      "range": {
        "startLine": 8,
        "startChar": 38,
        "endLine": 8,
        "endChar": 42
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "664fb173_62c6b633",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 50,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "That seems like a lovely win - as there is no way today to validate that a node label is real or not.",
      "range": {
        "startLine": 48,
        "startChar": 48,
        "endLine": 50,
        "endChar": 14
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "540230a8_0f4c4503",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 50,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Yeah, that can become a config error now instead of a runtime error.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f2b262c7_1179d80e",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 57,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "FWIW - maybe another point, or maybe not - is that people have from time to time expressed a desire to use something other than dib to build images. Zuul ultimately doesn\u0027t care what is used to create an image file - and Zuul jobs can kind of do whatever you want them to do. So it adds flexibility without also having to invent a new execution mechanism in nodepool.",
      "range": {
        "startLine": 57,
        "startChar": 34,
        "endLine": 57,
        "endChar": 61
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "03c06ac9_23673d97",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 57,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Yes absolutely; I figure we\u0027ll have roles in zuul-jobs for running diskimage builder, but folks can do whatever to split out an image file.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6039229c_2bb01bff",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 59,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "Yolanda would be so happy.",
      "range": {
        "startLine": 59,
        "startChar": 21,
        "endLine": 59,
        "endChar": 48
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "70df031c_ea715c9e",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 106,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "The case described below is about images missing from zuul\u0027s zk state. There isn\u0027t anything for discovering that a remote image is missing when we had previously uploaded it, updating the state and re-emitting the zuul trigger, is there?",
      "range": {
        "startLine": 105,
        "startChar": 65,
        "endLine": 106,
        "endChar": 8
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "3062b93c_0001a2f6",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 106,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Correct, I wasn\u0027t anticipating doing that.  But I think that would be possible.  We can have it periodically audit all parts of the system, and if an upload is missing, re-upload (and revalidate it -- but we\u0027d have to be careful to make sure the \"in-service\" flag is a one-way state transition -- we don\u0027t take images out of service once they go in).",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "52909554_09b8c043",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 127,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "Some image formats are harder than others. Would it be possible to have a parent job build the raw image and a child job do the raw-to-format conversion? Thinking about using a fedora node to build a fedora image and then an ubuntu node to convert it to vhd. Granted - the fedora image could just podman run a vhd-util container",
      "range": {
        "startLine": 124,
        "startChar": 54,
        "endLine": 127,
        "endChar": 24
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "2284f436_ad6e6f3d",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 127,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Oh interesting.  I think that would work with what\u0027s written here without any changes.  You\u0027d just make sure both the parent and child jobs had the image name matcher so they both run, then the parent job can return the raw image artifact, and the child job can fetch that and do the conversion and return the vhd artifact.  The zuul reporter would look at all the jobs for artifacts, so it would see both.  And if the parent job returned an intermediate artifact the child job used but the zuul driver isn\u0027t configured to look for, that\u0027s fine too, it\u0027ll just ignore that.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d1628ca6_f2bda5ee",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 181,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "This could get fun with builder nodes being far away from the storage system that is near to the zuul-launcher.",
      "range": {
        "startLine": 181,
        "startChar": 25,
        "endLine": 181,
        "endChar": 67
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "e38f45f5_2b1b0984",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 181,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Yes, there may be more really long network transfers in this system than we see right now with the builders.  A site like opendev with multiple options for storing image files might want to carefully select what object storage to use for them.  Also consider gzipping them and uploading them with an appropriate content-encoding header, etc.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6e71a4b9_e052a7a6",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 224,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "Does an image with a failed validation indicate to zuul that it should emit the event for needing one of these images?",
      "range": {
        "startLine": 224,
        "startChar": 1,
        "endLine": 224,
        "endChar": 29
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a36a7099_03d699ee",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 224,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Yes, I think at that point we should emit an image-delete event, probably wait until that image is cleaned up (so that we don\u0027t pile up a bunch of bad images in the object storage system) then emit another build-image event.\n\nPerhaps there should be a configurable interval too, so we might wait an extra X hours?",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "45579aac_abaddfb8",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 277,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "This spec describes a build job storing the location of the image in an artifact. Potentially instead of an artifact, in the case of snapshots, it could store the location of the image id in the cloud? Then the upload step can be a no-op but could trigger the validation workflow and the internal data upkeep.\n\nThis could likely be very achievable for people with only one cloud.",
      "range": {
        "startLine": 277,
        "startChar": 0,
        "endLine": 277,
        "endChar": 34
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "3a46c6b1_10c14dcc",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 277,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Well, what I was trying to get at here is that we should let zuul-launcher perform the actual API call to make the snapshot.  So the job runs on a node and does a bunch of stuff to the filesystem.  Then, instead of having the job call \"openstack image create\" or whatever (since that would need credentials in a zuul secret), we would just have the job end, and then zuul-launcher will perform that api call before deleting the node.  Then that image location gets stored in ZK.\n\nMostly, I described the above so that Zuul secrets with cloud creds aren\u0027t needed in the job.\n\nHaving said all that, I agree your process would work as well, with fewer changes needed in Zuul, just with the caveat about the creds needed to make the snapshot.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "5edc4133_7d1c4778",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 277,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T21:51:19Z",
      "side": 1,
      "message": "Ah - totally - that\u0027s a great point. I think I was thinking about things like packer where the zuul node would be where you\u0027d run packer - but you\u0027d already need creds for packer to create _another_ VM to specialize. I like your version better, of course, because I don\u0027t like packer in the first place.",
      "parentUuid": "3a46c6b1_10c14dcc",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d86a4e6e_8e1933f9",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 356,
      "author": {
        "id": 2
      },
      "writtenOn": "2022-07-14T18:27:21Z",
      "side": 1,
      "message": "awww. you know it would be totally cool to have job use an new image and depend-on the change to add the image to the system. ;)",
      "range": {
        "startLine": 356,
        "startChar": 0,
        "endLine": 356,
        "endChar": 16
      },
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "99f75ecf_856173f9",
        "filename": "doc/source/developer/specs/nodepool-in-zuul.rst",
        "patchSetId": 1
      },
      "lineNbr": 356,
      "author": {
        "id": 1
      },
      "writtenOn": "2022-07-14T21:20:03Z",
      "side": 1,
      "message": "Yeah it would... I\u0027ve been thinking about that and it just seems messy though because we still want images to be \"global\".\n\nI mean, we could totally have a repo where we have \"image\" stanzas like below.  Let\u0027s use opendev as an example.  We could have \"opendev/images\" with that configuration in it.  And then we could include opendev/images in every tenant (but with \u0027include: [image]\u0027 in most tenants, and only include jobs, etc, in the \"opendev\" tenant).  Then every tenant gets access to those images.  If a tenant has extra \"image\" definitions, it gets access to those extra images as well.  For example, let\u0027s say we have an \"openstack/images\" repo where we make special devstack images for openstack.  So far so good.\n\nSo what if we have:\n\nopendev/images: has a \"debian-unstable\" image\nopenstack/images: has a \"devstack\" image\nzuul/images: decides to define its own \"devstack\" image\n\nSince our image names are global, we have a name conflict and it\u0027s a bit tricky to resolve because the actual image exists outside of the context of a tenant (I\u0027m taking it for granted we don\u0027t want to build/upload copies of images for each tenant).\n\nMaybe what we could do is, internally, we go ahead and use the canonical name.  So in ZooKeeper, we store image information at:\n\n/zuul/images/opendev.org%2fopendev%fimages%2fdebian-unstable/...\n\nAnd then when we go to run a job, we use the configuration objects in that tenant to resolve the label down to an image, and whatever repo defines that devstack image we use the full canonical image name.  So if the job says to use the \"devstack\" label and the \"devstack\" label says to use the \"devstack\" image, if that job is running in the openstack tenant, it\u0027s going to get the \"opendev.org/openstack/images/devstack\" and if a job runs in the zuul tenant, it\u0027s going to get \"opendev.org/zuul/images/devstack\".  Which is probably what each expects.\n\nThat might actually work.",
      "revId": "032f625fd751f08ee572babe681088aa418c0447",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    }
  ]
}