{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "2cfcd502_4d89cb97",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 17,
      "author": {
        "id": 1
      },
      "writtenOn": "2024-05-13T22:41:17Z",
      "side": 1,
      "message": "To clarify, the current version of the spec does not have a thundering herd problem either.  It avoids that by having two loops: a global request assignment loop, then individual provider loops.  Much like the current zuul event dispatch system.\n\nIf I understand your proposal correctly, this would allow multiple launchers to run the global request assignment loop simultaneously; that reduces a potential bottleneck in processing.\n\nOne of the goals of the spec was to be able to be more deliberate about which providers handle which requests, so I think having multiple launchers run the global assignment loop simultaneously but using implicit coordination to decide which requests they handle makes sense.  The actual substance of the assignment loop (deciding which providers should handle requests based on some limited things now (such as available quota and types) and possibly more complex ideas later (like costs, time of day, anything else) remains.\n\nIt also removes the per-provider lock (see below for more about that).",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f8372faa_1ab447d4",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 17,
      "author": {
        "id": 27582
      },
      "writtenOn": "2024-05-14T06:48:50Z",
      "side": 1,
      "message": "Ack. And yes, only the current Nodepool launcher implementation has a thundering herd problem. Sorry for not being clear about that.",
      "parentUuid": "2cfcd502_4d89cb97",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1cb0c47e_5de91b87",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 26,
      "author": {
        "id": 27582
      },
      "writtenOn": "2024-05-14T06:48:50Z",
      "side": 1,
      "message": "As Jim pointed out in his comment above: only the current Nodepool launcher implementation has this thundering herd problem.",
      "range": {
        "startLine": 26,
        "startChar": 48,
        "endLine": 26,
        "endChar": 71
      },
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d0616bd1_3f4ec2e3",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 30,
      "author": {
        "id": 27582
      },
      "writtenOn": "2024-05-08T13:22:18Z",
      "side": 1,
      "message": "The check whether there is any remaining quota is currently done independently for each node request based on cached information about existing nodes. This means that concurrently processed requests will not consider each other\u0027s resource usage and there might be a small delay until new nodes show up in the cache.\n\nSame is true for the tenant quota that also considers resources used by other providers, so there can be more requests in parallel that might not consider each other\u0027s resource requirements.\n\nWith the proposed non-coordinating launcher approach the main difference would be that the possibility for quota races would increase when scaling up the launcher instances (more requests processed in parallel). The accuracy would probably be similar to the that of the tenant quota today.\n\nWays to account for that effect on calculated quota (in order of preference):\n- provider quota calculation is best-effort similar to the tenant quota today; consider/document potential \"overshoot\" when setting resource limits\n- calculate quota when assigning requests to providers, but also re-calculate/check the quota limits on provider level and reject a request if it would exceed the quota limits (used quota might still be a bit off)\n- use a combination of coordination (lock) for the global request queue (inc. quota calculation) as proposed in the spec and non-coordinating for provider processing as proposed in this change (this might allow more precise quota calculations than what we have today)\n- synchronize quota reservation/calculation (optional?; might slow down overall request processing)",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a9504f7d_4013d04a",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 30,
      "author": {
        "id": 1
      },
      "writtenOn": "2024-05-13T22:41:17Z",
      "side": 1,
      "message": "In your first paragraph, you say \"is currently done\" but do you mean \"this proposed update to the spec\"?  That\u0027s the only interpretation that makes sense to me, and it took me a while to get there, so I wanted to double check.\n\nI think that, if we decide this is worth doing, then the first option is acceptable.  I think I like the second one better since it gets things moving faster and is more robust.  It probably needs the idea of a \"temporary rejection\".\n\nSeparate, but related: this update also proposes removing the per-provider lock.\n\nThat has implications related to quota (as you note) and also rate limits.  In many installations (perhaps most) a limiting factor is the cloud rate limit.  So any system that has more than one provider issuing cloud api calls will, in effect, have similar problems to the quota problem you describe.  The solutions may be similar:\n\n- accept that providers will hit rate limit and eventually recover\n- [temporarily] reject the request if the provider hits rate limit\n- [this one doesn\u0027t apply]\n- synchronize rate limit calculations (will slow overall request processing)\n\nOne way to look at this is that the current version of the spec has a global request lock in order to make correct quota calculations.  And it has a per-provider lock to make correct rate limit calculations.  These global locks slow down processing somewhat, but they allow us to do these calculations correctly with only the caveat that the quota or rate limit values need to be correct in zk once for each iteration (ie, for rate limit, record the current rate info at the end of the loop and restore it at the start).  Removing those locks means we either need to store a lot more data in zk (option #4) or accept some slop.",
      "parentUuid": "d0616bd1_3f4ec2e3",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "3d9b0f68_ba43a380",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 30,
      "author": {
        "id": 27582
      },
      "writtenOn": "2024-05-14T06:48:50Z",
      "side": 1,
      "message": "With \"currently done\" I was talking about the quota implementation as it is today in Nodepool. My comment was comparing the current way of quota handling in Nodepool with how the quota COULD be handled when using the approach proposed here.\n\nThe change as-is doesn\u0027t have any kind of quota handling atm. as I was mainly focusing on the way requests are handled and comparing/contrasting them as good as possible. If we think that might be worth prototyping I\u0027m happy to do that.\n\nRe. rate limiting we already have a per-launcher rate limit in the current Nodepool implementation. I\u0027d assume that when running multiple launcher instances for the same provider this setting would need to be adjusted accordingly in case of multiple launchers for the same provider.\n\nOf course, a more sophisticated provider rate limit implementation could tune the request rate dynamically or we could handle rate limit error as you proposed, but I\u0027m not sure if it\u0027s worth doing.\n\nIf launchers frequently hit the provider rate limit and those limits can not be increased, there is probably not point in running X number of launchers. So I think the main concern here might be bursts of requests which I think could be handled with some kind of static provider per-launcher rate-limit.\n\nMaybe one option could be to configure a static per-provider rate limit that is then shared between the online provider launchers (e.g. based on the info from the component registry).",
      "parentUuid": "a9504f7d_4013d04a",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8e0fdadc_b84a129f",
        "filename": "/COMMIT_MSG",
        "patchSetId": 2
      },
      "lineNbr": 30,
      "author": {
        "id": 1
      },
      "writtenOn": "2024-05-14T13:28:37Z",
      "side": 1,
      "message": "Oh! Then in that case, regarding your first paragraph in the original comment: there is no provider quota race problem in nodepool today because every provider is handled by only one launcher (running more than one launcher with a single provider is off-label and not supported).\n\nBut yes to what you wrote in the second paragraph: tenant quota today is slightly racy, and that is a known design compromise.\n\nI\u0027ve been considering this change an exercise in evaluating a proposed change to the spec, so I think the best target of comparison is to that spec.  We\u0027ve already decided to approve the spec, which achieves cooperative launching via explicit coordination and locking.  That improves the speed of request handling, as well as giving us options for improving the behavior, while maintaining the accuracy of quota and rate calculations.  I see it as this change is to evaluate a further revision of that where we would potentially loosen those guarantees in exchange for more speed.\n\n\u003e The change as-is doesn\u0027t have any kind of quota handling atm. as I was mainly focusing on the way requests are handled and comparing/contrasting them as good as possible. If we think that might be worth prototyping I\u0027m happy to do that.\n\nI don\u0027t think this change needs to actually prototype the quota handling, I thought you just wrote this to benchmark speed.  We would only need to consider the speed of quota handling if we were to do something like continuously update quota values (option #4).  Neither of us want to do that.\n\n\u003e Re. rate limiting we already have a per-launcher rate limit in the current Nodepool implementation. I\u0027d assume that when running multiple launcher instances for the same provider this setting would need to be adjusted accordingly in case of multiple launchers for the same provider.\n\nNo, the design in the spec does not need adjusting like that, because the design in the spec never has more than one launcher drive a provider at the same time.  There is a provider lock, which means that it can perform accurate rate limit calculations for each provider, even across multiple launchers.",
      "parentUuid": "3d9b0f68_ba43a380",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a1ca923b_9aa35513",
        "filename": "zuul/zk/launcher.py",
        "patchSetId": 2
      },
      "lineNbr": 317,
      "author": {
        "id": 1
      },
      "writtenOn": "2024-05-13T22:41:17Z",
      "side": 1,
      "message": "Spec says all.",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1b1b2bc1_0d845c6b",
        "filename": "zuul/zk/launcher.py",
        "patchSetId": 2
      },
      "lineNbr": 317,
      "author": {
        "id": 27582
      },
      "writtenOn": "2024-05-14T06:48:50Z",
      "side": 1,
      "message": "Acknowledged",
      "parentUuid": "a1ca923b_9aa35513",
      "revId": "51f138889bd02443f81a2bb3c9e9350605641459",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    }
  ]
}