{
  "comments": [
    {
      "key": {
        "uuid": "5f7c97a3_26da7278",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 67,
      "author": {
        "id": 27900
      },
      "writtenOn": "2018-07-06T07:27:25Z",
      "side": 1,
      "message": "use of `k8s_raw` is deprecated in favor of `k8s` (ansible 2.6)",
      "range": {
        "startLine": 67,
        "startChar": 0,
        "endLine": 67,
        "endChar": 7
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_873d5f2d",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 76,
      "author": {
        "id": 2
      },
      "writtenOn": "2018-07-09T13:59:16Z",
      "side": 1,
      "message": "container images and vm images have a different costs and no consistent naming scheme. For vm images, we define how to build and upload them for the nodepool builders, then we define nodepool labels that abstract away the details of the actual cloud image name, allowing use to have a label like \u0027ubuntu-xenial\u0027 and an actual image on each cloud with a much more complex name.\n\nWith containers, it feels like requiring a nodepool admin to create (or review/approve) a label for a container image in the nodepool config is admin work that doesn\u0027t provide any value. It also doesn\u0027t allow for experimentation, since the nodepool config is not speculative, so in order to write a zuul job that uses the node:slim image, I\u0027d need to go make a label in nodepool.yaml, get it landed, then make a zuul job that uses it. Then, if it turns out I needed the node image or the node:alpine image - or the node:slim-8.5 image, I need to go make nodepool labels for each of them. (incidentally, for openstack, I\u0027d need to, I believe, make those labels for every cloud we have, assuming we\u0027re running a k8s in every cloud region) \n\nWhat about adding another field to the nodeset construct that is mutually exclusive with label that is something like:\n\n  - nodeset:\n      nodes:\n        - name: contoller\n          container: python:3.6\n\nthat can pull and boot any container image by reference?\n\nThen, in the nodepool config, for a given provider - or maybe a pool - we could add config information indicating that the provider/pool has the capabilities of providing containers, so when a request for container: python:3.6 comes in, the nodepool scheduler can check to see which provider can boot a container at all, then just boot the pod with the appropriate things.",
      "range": {
        "startLine": 76,
        "startChar": 0,
        "endLine": 76,
        "endChar": 57
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_fd9f0462",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 76,
      "author": {
        "id": 1
      },
      "writtenOn": "2018-07-09T15:00:46Z",
      "side": 1,
      "message": "The two are not so different.  It is anticipated that nodepool may build container images as well, for the same reasons that we build VM images: to cache git repos and ensure OS packages are present.  See line 243.\n\nFor the container-as-machine case, I think we should maintain parity between the two systems -- the difference is not really meant to be user-visible.\n\nThis idea is a good one, however.  Perhaps we should discuss it further, but we should also discuss applying the same thing to cloud images (I can see no argument for this for container images that doesn\u0027t also apply to cloud images), as well as how to restrict it (it\u0027s certainly valid for an admin to want to restrict what container images are available to jobs).",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_7d6bf41d",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 76,
      "author": {
        "id": 2
      },
      "writtenOn": "2018-07-09T15:11:41Z",
      "side": 1,
      "message": "I\u0027m going to ignore the cached resources part first for a sec. I\u0027ll come back to it, I promise.\n\nThe reason I think that container images are different from vm images is that there is currently a defacto global registry of  container names that can be used from anywhere that are not cloud-vendor specific. The newer technologies that aim to be more decentralized still allow image references to be urls - so a container image name can be specified by a user and the vendor of a cloud doesn\u0027t have an opportunity to mess things up.\n\nVM images, otoh, are different on every cloud. There is no \"ubuntu:xenial\" image that can be counted on to be ubuntu:xenial - so our vm image labels provide value in that they allow us to describe a generic concept and map it to specific image implementation names. This is both for hiding different names for pre-existing cloud provider images, as well as our serial number named images produced by nodepool-builder.\n\nBack to cached resources ...\n\nSince we\u0027re talking about always booting a pod-of-one-container, to deal with caching without needing to rebuild container images (or without having to build derivitive images) - we could alternately make a container image with the cached git repos in it (and other cached content) and have nodepool be configured to boot a pod with the requested image as well as the repo cache image with directories from the cache container bind-mounted in to the requested image as volumes. Since containers in pods can share a filesystem, this is a way we could provide the same functionality we provide to vm images.\n\nIt would also be much more efficient - since we\u0027d otherwise be laying the caching on TOP of the requested base image - which means if we made 5 different container images with added cache we\u0027d have 5 complete copies of the cache layer in the docker image cache. With a cache image that\u0027s just there to provide directories, the cache layer can be in the image layer cache once on each k8s node, and then the requested container image itself can be used over and over again unmodified.",
      "parentUuid": "5f7c97a3_fd9f0462",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_9a812dfd",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 76,
      "author": {
        "id": 1
      },
      "writtenOn": "2018-07-09T20:56:37Z",
      "side": 1,
      "message": "Okay, we\u0027ve got two suggestions here:\n\n1) Possibly using a two-container pod with the second container only holding the cached repos so that the image for the first container can be more vanilla.  Let\u0027s look into whether that would work, because if so, that seems like a better approach rather than building new images for each upstream image just to add the local cache.\n\n2) Specifying containers from Zuul.  If we agree (and I think we do) that there\u0027s still a place for nodepool providing a label-to-image mapping for containers (so that a local admin can provide their own local container images, or add certain local information (possibly the thing in #1 even), or restrict access to only defined container images, then I think the spec as written stands.  I think the suggestion to dynamically request container images from Zuul is a good one, but it opens some questions about implementation, usage, and access control which we should think about carefully and may distract us from the immediate goal.  So I\u0027d like to consider that separately later.\n\nHow\u0027s that sound?",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_17e3eba1",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 79,
      "author": {
        "id": 9311
      },
      "writtenOn": "2018-07-05T22:48:34Z",
      "side": 1,
      "message": "How are zuul-executors supposed to authenticate with the cluster?",
      "range": {
        "startLine": 78,
        "startChar": 34,
        "endLine": 79,
        "endChar": 16
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_27d56b1b",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 79,
      "author": {
        "id": 2
      },
      "writtenOn": "2018-07-09T13:59:16Z",
      "side": 1,
      "message": "I do not believe they are supposed to authenticate with the cluster in this context- I believe this is supposed to be nodepool doing the k8s/openshift api calls, creating the pod and handing it to the executors in the ansible inventory.",
      "parentUuid": "5f7c97a3_17e3eba1",
      "range": {
        "startLine": 78,
        "startChar": 34,
        "endLine": 79,
        "endChar": 16
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_7d021440",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 79,
      "author": {
        "id": 9311
      },
      "writtenOn": "2018-07-09T15:16:17Z",
      "side": 1,
      "message": "Could you define \"handling it to the executors\"? It seems like the ansible-playbook runners needs a .kube/config or kubectl_cert_file or  kubectl_token or *something* to be able to use kubectl exec and run tasks on any inventory pods. Would be nice to know more about this and how it qualifies as \"little customization expected here\" :)",
      "parentUuid": "5f7c97a3_27d56b1b",
      "range": {
        "startLine": 78,
        "startChar": 34,
        "endLine": 79,
        "endChar": 16
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_3a7c991e",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 79,
      "author": {
        "id": 1
      },
      "writtenOn": "2018-07-09T20:56:37Z",
      "side": 1,
      "message": "Yes, the executors will need some credentials.  Nodepool will provide the credentials to the executors via zookeeper (much in the way it provides other connection information now, such as the connection plugin and ip address (and if we were designing it from scratch now, we might even have it provide the ssh key; we still may in the future)).  I expect that the best way to do that would be for nodepool to create a service account and pass the resulting token to zuul via zookeeper.  I could add this to the spec if you like, or if you have another suggestion, I could incorporate that.  Though I\u0027m happy to leave some flexibility here for implementation -- if we communicate the idea that nodepool should provide whatever zuul needs to run kubectl, I\u0027m happy.\n\nBy \"very little customization\", I mean that the nodepool configuration language is not designed to replace or re-implement the k8s api.  If we find ourselves adding more than the minimum of information needed to bring up a usable image to run a pep8 job to the nodepool config, then we should rethink what we\u0027re doing.",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_27c9eb23",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 96,
      "author": {
        "id": 2
      },
      "writtenOn": "2018-07-09T13:59:16Z",
      "side": 1,
      "message": "Related to my earlier comment, the \u0027this has ssh\u0027 vs. \u0027this uses kubectl\u0027 seems like it would need to be a config flag somewhere. (whether it\u0027s in a label definition, or in a nodeset definition)\n\nHowever, have we identitied environments where people would want to use k8s/containers as build resources that specifically want to ssh into the containers rather than using the inventory/kubectl approach you list below? As you mention, it\u0027s rather complicated - and I\u0027d hate to take on the complexity if it\u0027s not really a thing anyone wants.",
      "range": {
        "startLine": 95,
        "startChar": 24,
        "endLine": 96,
        "endChar": 48
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_1da3e0b2",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 96,
      "author": {
        "id": 1
      },
      "writtenOn": "2018-07-09T15:00:46Z",
      "side": 1,
      "message": "Yes, it would be a flag in nodepool (either a label or, if nodepool builds the image, an image flag).\nLines 109-111 say we\u0027re not going to do this right now.  This is just collecting the information we\u0027ve generated as part of this process for future work if someone wants to take it on, as well as providing contect for why we\u0027re doing the other thing (this was, after all, one of the most important questions we set out to answer about how to do this).",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_9d4ff0bc",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 96,
      "author": {
        "id": 2
      },
      "writtenOn": "2018-07-09T15:11:41Z",
      "side": 1,
      "message": "++",
      "parentUuid": "5f7c97a3_1da3e0b2",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_e7ce932a",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 111,
      "author": {
        "id": 2
      },
      "writtenOn": "2018-07-09T13:59:16Z",
      "side": 1,
      "message": "++",
      "range": {
        "startLine": 109,
        "startChar": 38,
        "endLine": 111,
        "endChar": 56
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_a65d22ed",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 160,
      "author": {
        "id": 27900
      },
      "writtenOn": "2018-07-06T07:27:25Z",
      "side": 1,
      "message": "as before - k8s, instead of k8s_raw",
      "range": {
        "startLine": 160,
        "startChar": 16,
        "endLine": 160,
        "endChar": 38
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_066416b2",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 181,
      "author": {
        "id": 27900
      },
      "writtenOn": "2018-07-06T07:27:25Z",
      "side": 1,
      "message": "k8s",
      "range": {
        "startLine": 181,
        "startChar": 51,
        "endLine": 181,
        "endChar": 58
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_461c4e12",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 213,
      "author": {
        "id": 27900
      },
      "writtenOn": "2018-07-06T07:27:25Z",
      "side": 1,
      "message": "k8s",
      "range": {
        "startLine": 213,
        "startChar": 5,
        "endLine": 213,
        "endChar": 12
      },
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_c6be1ed3",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 226,
      "author": {
        "id": 27900
      },
      "writtenOn": "2018-07-06T07:27:25Z",
      "side": 1,
      "message": "https://docs.ansible.com/ansible/latest/modules/k8s_module.html (note, k8s is present only from 2.6, while k8s_raw only in 2.5. https://github.com/openshift/openshift-restclient-python guys have deprecated k8s_raw. Instead `k8s` is able to handle k8s and openshift resources with one module",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_bda50cb6",
        "filename": "doc/source/developer/specs/container-build-resources.rst",
        "patchSetId": 4
      },
      "lineNbr": 226,
      "author": {
        "id": 1
      },
      "writtenOn": "2018-07-09T15:00:46Z",
      "side": 1,
      "message": "k8s_raw handles openshift and k8s as well.",
      "revId": "1b2b1d0b8c34774df3012f7390311021c59b3642",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    }
  ]
}